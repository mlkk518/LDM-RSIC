17:10:44 INFO - logger_setup: K:\code_demo\1image_compression\high-fidelity-generative-compression-master\high-fidelity-generative-compression-master\compress.py
17:11:36 INFO - logger_setup: K:\code_demo\1image_compression\high-fidelity-generative-compression-master\high-fidelity-generative-compression-master\compress.py
17:14:39 INFO - logger_setup: K:\code_demo\1image_compression\high-fidelity-generative-compression-master\high-fidelity-generative-compression-master\compress.py
17:16:34 INFO - logger_setup: K:\code_demo\1image_compression\high-fidelity-generative-compression-master\high-fidelity-generative-compression-master\compress.py
17:16:37 INFO - load_model: Loading model ...
17:16:37 INFO - load_model: MODEL TYPE: compression_gan
17:16:37 INFO - load_model: MODEL MODE: evaluation
17:16:37 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:16:37 INFO - load_model: Trainable parameters:
17:16:37 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:16:37 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:16:37 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:16:37 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:16:37 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:16:37 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:16:37 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:16:37 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:16:37 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:16:37 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:16:37 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:16:37 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:16:37 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:16:37 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:16:37 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:16:37 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:16:37 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:16:37 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:16:37 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:16:37 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:16:37 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:16:37 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:16:37 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:16:37 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:16:37 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:16:37 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:16:37 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:16:37 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:16:37 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:16:37 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:16:37 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:16:37 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:16:37 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:16:37 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:16:37 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:16:37 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:16:37 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:16:37 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:16:37 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:16:37 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:16:37 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:16:37 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:16:37 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:16:37 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:16:37 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:16:37 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:16:37 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:16:37 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:16:37 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:16:37 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:16:37 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:16:37 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:16:37 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:16:37 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:16:37 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:16:37 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
17:16:37 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
17:16:37 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
17:16:37 INFO - load_model: Number of trainable parameters: 148286543
17:16:37 INFO - load_model: Estimated model size (under fp32): 593.146 MB
17:16:37 INFO - load_model: Model init 3.686s
17:16:37 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 1, 'figures_save': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 5000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02', 'noise_dim': 32, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02', 'storage_save': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02/tensorboard', 'timestamp': '2020_08_26_04:11', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'checkpoints/norm_low_rate_openimages_compression_2020_08_19_16_13_epoch2_idx168720_2020_08_21_04:00.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='Pretrained_model/hific_low.pt', image_dir='images/', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='Pretrained_model/hific_low.pt', image_dir='images/', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=False)>, 'ckpt_path': 'Pretrained_model/hific_low.pt', 'image_dir': 'images/', 'metrics': False, 'output_dir': 'data/reconstructions', 'reconstruct': True}
17:16:37 INFO - compress_and_decompress: Building hyperprior probability tables...
17:16:50 INFO - compress_and_decompress: All tables built.
17:16:50 INFO - compress_and_decompress: Starting compression...
17:19:16 INFO - logger_setup: K:\code_demo\1image_compression\high-fidelity-generative-compression-master\high-fidelity-generative-compression-master\compress.py
17:19:20 INFO - load_model: Loading model ...
17:19:20 INFO - load_model: MODEL TYPE: compression_gan
17:19:20 INFO - load_model: MODEL MODE: evaluation
17:19:20 INFO - load_model: Model(
  (Encoder): Encoder(
    (pre_pad): ReflectionPad2d((3, 3, 3, 3))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((1, 1, 1, 1))
    (conv_block1): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block2): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block3): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block4): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block5): Sequential(
      (0): ReflectionPad2d((0, 1, 1, 0))
      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)
      (2): ChannelNorm2D()
      (3): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((1, 1, 1, 1))
      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))
    )
  )
  (Generator): Generator(
    (pre_pad): ReflectionPad2d((1, 1, 1, 1))
    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))
    (post_pad): ReflectionPad2d((3, 3, 3, 3))
    (conv_block_init): Sequential(
      (0): ChannelNorm2D()
      (1): ReflectionPad2d((1, 1, 1, 1))
      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))
      (3): ChannelNorm2D()
    )
    (resblock_0): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_1): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_2): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_3): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_4): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_5): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (resblock_6): ResidualBlock(
      (pad): ReflectionPad2d((1, 1, 1, 1))
      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))
      (norm1): ChannelNorm2D()
      (norm2): ChannelNorm2D()
    )
    (upconv_block1): Sequential(
      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block2): Sequential(
      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block3): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (upconv_block4): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (1): ChannelNorm2D()
      (2): ReLU()
    )
    (conv_block_out): Sequential(
      (0): ReflectionPad2d((3, 3, 3, 3))
      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))
    )
  )
  (Hyperprior): Hyperprior(
    (analysis_net): HyperpriorAnalysis(
      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)
    )
    (synthesis_mu): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (synthesis_std): HyperpriorSynthesis(
      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (hyperlatent_likelihood): HyperpriorDensity()
    (hyperprior_entropy_model): HyperpriorEntropyModel(
      (distribution): HyperpriorDensity()
    )
    (prior_density): PriorDensity()
    (prior_entropy_model): PriorEntropyModel(
      (distribution): PriorDensity()
    )
  )
  (squared_difference): MSELoss()
  (perceptual_loss): PerceptualLoss()
)
17:19:20 INFO - load_model: Trainable parameters:
17:19:20 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])
17:19:20 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])
17:19:20 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])
17:19:20 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])
17:19:20 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])
17:19:20 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])
17:19:20 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])
17:19:20 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])
17:19:20 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])
17:19:20 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])
17:19:20 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])
17:19:20 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])
17:19:20 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])
17:19:20 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])
17:19:20 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])
17:19:20 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])
17:19:20 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])
17:19:20 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])
17:19:20 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])
17:19:20 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])
17:19:20 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])
17:19:20 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])
17:19:20 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])
17:19:20 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])
17:19:20 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])
17:19:20 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])
17:19:20 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])
17:19:20 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])
17:19:20 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])
17:19:20 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])
17:19:20 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])
17:19:20 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])
17:19:20 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])
17:19:20 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])
17:19:20 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])
17:19:20 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])
17:19:20 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])
17:19:20 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])
17:19:20 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])
17:19:20 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])
17:19:20 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])
17:19:20 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])
17:19:20 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])
17:19:20 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])
17:19:20 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])
17:19:20 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])
17:19:20 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])
17:19:20 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])
17:19:20 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])
17:19:20 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])
17:19:20 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])
17:19:20 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])
17:19:20 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])
17:19:20 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])
17:19:20 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])
17:19:20 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])
17:19:20 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])
17:19:20 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])
17:19:20 INFO - load_model: Number of trainable parameters: 148286543
17:19:20 INFO - load_model: Estimated model size (under fp32): 593.146 MB
17:19:20 INFO - load_model: Model init 3.664s
17:19:20 INFO - compress_and_decompress: {'batch_size': 1, 'beta': 0.15, 'checkpoints_save': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': 'data/openimages', 'discriminator_steps': 1, 'figures_save': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02/figures', 'force_set_gpu': False, 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 5000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 949742, 'n_epochs': 10, 'n_residual_blocks': 7, 'n_steps': 1000000, 'name': 'norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02', 'noise_dim': 32, 'normalize_input_image': True, 'regime': 'low', 'sample_noise': False, 'save': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02', 'storage_save': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/norm_low_rate_gan_v1_openimages_compression_gan_2020_08_22_16_02/tensorboard', 'timestamp': '2020_08_26_04:11', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'checkpoints/norm_low_rate_openimages_compression_2020_08_19_16_13_epoch2_idx168720_2020_08_21_04:00.pt', 'weight_decay': 1e-06, '_get_args': <bound method _AttributeHolder._get_args of Namespace(ckpt_path='Pretrained_model/hific_low.pt', image_dir='images/', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=False)>, '_get_kwargs': <bound method _AttributeHolder._get_kwargs of Namespace(ckpt_path='Pretrained_model/hific_low.pt', image_dir='images/', output_dir='data/reconstructions', batch_size=1, reconstruct=True, save=False, metrics=False)>, 'ckpt_path': 'Pretrained_model/hific_low.pt', 'image_dir': 'images/', 'metrics': False, 'output_dir': 'data/reconstructions', 'reconstruct': True}
17:19:20 INFO - compress_and_decompress: Building hyperprior probability tables...
17:19:30 INFO - compress_and_decompress: All tables built.
17:19:30 INFO - compress_and_decompress: Starting compression...
17:19:41 INFO - compress_and_decompress: Complete. Reconstructions saved to data/reconstructions. Output statistics saved to data/reconstructions\compression_metrics.h5
17:19:41 INFO - compress_and_decompress: Time elapsed: 11.240 s
17:19:41 INFO - compress_and_decompress: Rate: 0.089 Images / s:
